#!/usr/bin/env python

from collections import OrderedDict
from collections import namedtuple
from uuid import getnode as get_mac
import argparse
import shutil
import glob
import itertools
import json
import os
import re
import string
import subprocess
import sys
import time
from urllib.request import urlopen
import warnings
import zipfile

root_path = os.path.abspath('.')
default_json_file_path = root_path + '/cppdock.json'
default_dep_path = "https://github.com/"
ServiceFn = namedtuple('ServiceFn', ['nginx_proxy', 'postgres'])
noop = lambda *x: None

def remove_none(obj):
  if isinstance(obj, (list, tuple, set)):
    return type(obj)(remove_none(x) for x in obj if x is not None)
  elif isinstance(obj, dict):
    return type(obj)((remove_none(k), remove_none(v))
      for k, v in obj.items() if k is not None)
  else:
    return obj

def get_commands():
    return {
        'init':                 (command_init,              parse_args_init),
        'build':                (command_build,             parse_args_build),
        'release':              (command_release,           parse_args_release),
        'gen_certs':            (command_gen_certs,         parse_args_gen_certs),
        'dev':                  (command_dev,               parse_args_dev),
        'dev_cluster':          (command_dev_cluster,       parse_args_dev_cluster),
        'dev_registry':         (command_dev_registry,      parse_args_dev_registry),
        'install_dep':          (command_install_dep,       parse_args_install_dep),
        'install_src':          (command_install_src,       parse_args_install_src),
        'init_dev_service':     (command_init_dev_service,  parse_args_init_dev_service),
        'cluster_build':        (command_cluster_build,     parse_args_cluster_build),
        'cluster_sync':         (command_cluster_sync,      parse_args_cluster_sync),
        'cluster_start':        (command_cluster_start,     parse_args_cluster_start),
        'cluster_stop':         (command_cluster_stop,      parse_args_cluster_stop),
        'cluster_rm':           (command_cluster_rm,        parse_args_cluster_rm),
        'clean':                (command_clean,             parse_args_clean),
        '_complete':            (command_complete,          lambda _ : os.sys.argv[2]),
    }

def get_subcommands():
    return filter(lambda x: not x.startswith('_'), get_commands().keys())

def parse_args_command(args):
    parser = argparse.ArgumentParser(add_help=False)
    parser.add_argument('command', choices = get_commands().keys())
    return parser.parse_args(args)

def create_args_parser(subcommand):
    parser = argparse.ArgumentParser(prog='cppdock ' + subcommand)
    parser.add_argument('-f', '--file', dest      = 'json_file_path',
                                        default   = default_json_file_path,
                                        help      = 'Specify path to json file')
    return parser

def parse_args_init(args):
    parser = argparse.ArgumentParser(prog='cppdock init')
    parser.add_argument('-f', '--file', dest      = 'json_file_path',
                                        default   = default_json_file_path,
                                        help      = 'Specify path to json file')
    parser.add_argument('--print-config',
                                        dest      = 'print_config',
                                        action    = 'store_true',
                                        help      = 'Print consolidated config')

    return parser.parse_args(args)

def parse_args_build(args):
    parser = argparse.ArgumentParser(prog='cppdock build')
    parser.add_argument('-f', '--file', dest      = 'json_file_path',
                                        default   = default_json_file_path,
                                        help      = 'Specify path to json file')

    parser.add_argument('--print-only', dest      = 'print_only',
                                        action    = 'store_true',
                                        help      = 'Print Dockerfile without building')
    parser.add_argument('platform',     default   = None,
                                        nargs     = '?',
                                        help      = 'Name of target platform')

    return parser.parse_args(args)

def parse_args_release(args):
    parser = argparse.ArgumentParser(prog='cppdock release')
    parser.add_argument('-f', '--file', dest      = 'json_file_path',
                                        default   = default_json_file_path,
                                        help      = 'Specify path to json file')
    parser.add_argument('--push',       dest      = 'push',
                                        default   = None,
                                        help      = 'Tag name to push')
    parser.add_argument('platform',     default   = None,
                                        nargs     = '?',
                                        help      = 'Name of target platform')

    return parser.parse_args(args)

def parse_args_gen_certs(args):
    parser = argparse.ArgumentParser(prog='cppdock gen_certs')
    parser.add_argument('-f', '--file', dest      = 'json_file_path',
                                        default   = default_json_file_path,
                                        help      = 'Specify path to json file')
    parser.add_argument('platform',     default   = None,
                                        nargs     = '?',
                                        help      = 'Name of platform')

    return parser.parse_args(args)

def parse_args_dev(args):
    parser = argparse.ArgumentParser(prog='cppdock dev')
    parser.add_argument('-f', '--file', dest      = 'json_file_path',
                                        default   = default_json_file_path,
                                        help      = 'Specify path to json file')
    parser.add_argument('-p', '--port', dest      = 'port',
                                        help      = 'Specify port to `docker run`')
    parser.add_argument('--target',     dest      = 'target',
                                        default   = None,
                                        help      = 'Build target instead of shell')
    parser.add_argument('--shell',      dest      = 'shell',
                                        action    = 'store_true',
                                        help      = 'Forces shell after target builds')
    parser.add_argument('--shell-only', dest      = 'shell_only',
                                        action    = 'store_true',
                                        help      = 'Run only as an interactive shell')
    parser.add_argument('--privileged', dest      = 'privileged',
                                        action    = 'store_true',
                                        help      = 'Runs container with all host capabilities')

    group  = parser.add_argument_group('required arguments')
    group.add_argument('platform',      help      = 'Name of target platform')

    return parser.parse_args(args)

def parse_args_dev_cluster(args):
    parser = argparse.ArgumentParser(prog='cppdock dev_cluster')
    parser.add_argument('-f', '--file', dest      = 'json_file_path',
                                        default   = default_json_file_path,
                                        help      = 'Specify path to json file')
    parser.add_argument('--print-only', dest      = 'print_only',
                                        action    = 'store_true',
                                        help      = 'Print Compose file without deploying')
    parser.add_argument('--stop',       dest      = 'stop',
                                        action    = 'store_true',
                                        help      = 'Stops the dev cluster services')
    parser.add_argument('--clean',      dest      = 'clean',
                                        action    = 'store_true',
                                        help      = 'Stops the dev cluster services and deletes associated volumes')
    return parser.parse_args(args)

def parse_args_dev_registry(args):
    parser = create_args_parser('dev_registry')
    parser.add_argument('--stop',       dest      = 'stop',
                                        action    = 'store_true',
                                        help      = 'Stops the dev registry')
    return parser.parse_args(args)

def parse_args_install_dep(args):
    parser = argparse.ArgumentParser(prog='cppdock install_dep')
    group  = parser.add_argument_group('required arguments')
    group.add_argument('platform',      help      = 'Token for target platform')
    group.add_argument('branch',        help      = 'SHA1 of git revision')
    group.add_argument('repo',          help      = 'Name of git repo (e.g. boostorg/hana)')
    return parser.parse_args(args)

def parse_args_install_src(args):
    parser = argparse.ArgumentParser(prog='cppdock install_src')
    group  = parser.add_argument_group('required arguments')
    group.add_argument('branch',        help      = 'SHA1 of git revision')
    group.add_argument('repo',          help      = 'Name of git repo (e.g. llvm-mirror/clang)')
    group.add_argument('dest_dir',      help      = 'Path to copy source files to')
    group  = parser.add_argument_group('optional arguments')
    group.add_argument('src_dir',       nargs='?',
                                        default='.',
                                        help      = 'Path of subdirectory to copy from repo')
    return parser.parse_args(args)

def parse_args_init_dev_service(args):
    parser = argparse.ArgumentParser(prog='cppdock init_dev_service')
    parser.add_argument('--shell-only', dest      = 'shell_only',
                                        action    = 'store_true',
                                        help      = 'Run only as an interactive shell')
    parser.add_argument('--shell',      dest      = 'shell',
                                        action    = 'store_true',
                                        help      = 'Run as an interactive shell')
    parser.add_argument('--no-service', dest      = 'keep_alive',
                                        default   = True,
                                        action    = 'store_false',
                                        help      = 'Terminate container when process completes')
    group  = parser.add_argument_group('required arguments')
    group.add_argument('build_type',    help      = 'Build type')
    group.add_argument('target',        help      = 'Build target name')

    return parser.parse_args(args)

def parse_args_cluster_build(args):
    parser = create_args_parser('cluster_build')
    return parser.parse_args(args)

def parse_args_cluster_sync(args):
    parser = create_args_parser('cluster_sync')
    return parser.parse_args(args)

def parse_args_cluster_start(args):
    parser = create_args_parser('cluster_start')
    return parser.parse_args(args)

def parse_args_cluster_stop(args):
    parser = create_args_parser('cluster_stop')
    return parser.parse_args(args)

def parse_args_cluster_rm(args):
    parser = create_args_parser('cluster_rm')
    return parser.parse_args(args)

def parse_args_clean(args):
    parser = create_args_parser('clean')
    group  = parser.add_argument_group('required arguments')
    group.add_argument('clean_type',    help      = 'Type to specify subset of files or images to delete',
                                        choices   = ['build', 'install', 'images', 'all'])
    parser.add_argument('platform',     default   = None,
                                        nargs     = '?',
                                        help      = 'Name of target platform')
    return parser.parse_args(args)

def dispatch(command_args, cli_args):
    command = parse_args_command(command_args).command
    commands = get_commands()
    if command in commands:
        args = commands[command][1](cli_args)
        commands[command][0](args)

def command_complete(args_str):
    subcommands = get_subcommands()

    args = args_str.split();
    has_trailing_space = args_str.endswith(' ')
    # length includes a trailing empty string if there is a trailing space
    if args_str.endswith(' '):
        args.append('')
    # filter out completed options
    args = filter(lambda x: not x.startswith('-'), args[:-1]) + [args[-1]]
    length = len(args)

    if length < 2:
        # should not be possible from the completion script
        return

    candidates = []
    completion = ''

    if length == 2:
        # complete subcommand
        completion = args[1]
        candidates = subcommands
    elif length == 3:
        # complete platform
        subcommand = args[1]
        if subcommand in ['build', 'dev', 'release']:
            completion = args[2]
            config = load_config_file(default_json_file_path)
            candidates = get_all_platforms(config)
        if subcommand == 'clean':
            completion = args[2]
            candidates = ['all', 'build', 'install', 'release']
    elif length == 4:
        subcommand = args[1]
        if subcommand in ['clean']:
            completion = args[3]
            config = load_config_file(default_json_file_path)
            candidates = get_all_platforms(config)

    candidates = filter(lambda x: x.startswith(completion), candidates)
    print(' '.join(candidates))

def command_init(args):
    config = load_config_file_with_args(args)
    if args.print_config:
        print(json.dumps(config, indent=2, separators=(',', ': ')))

def command_build(args):
    config = load_config_file_with_args(args)
    set_master_machine(config)

    platforms = [args.platform] if args.platform else get_all_platforms(config)
    for platform in platforms:
        make_build_image(config, platform, args.print_only)

def command_release(args):
    config = load_config_file_with_args(args)
    config['__default_build_type'] = 'release'
    set_master_machine(config)
    print_only = False
    push_tag_name = args.push

    platforms = [args.platform] if args.platform else get_all_platforms(config)

    if push_tag_name and len(platforms) > 1:
        raise StandardError(
                "The `push` option is not allowed with multiple platforms")

    for platform in platforms:
        make_build_image(config, platform, print_only)
        install_dir = create_install_dir(config, platform)
        run_build_image_install(config, platform, install_dir)
        make_release_image(config, platform, install_dir)
        if push_tag_name:
            push_release_image(config, platform, push_tag_name)


def make_build_image(config, platform_name, print_only = False):
    if is_platform_service(config, platform_name):
        fn = make_service_image_fn(config, platform_name)
        fn(config, platform_name, print_only)
        return

    make_base_image(config, platform_name, print_only)
    dockerfile = make_deps_dockerfile(config, platform_name)
    tag_name = get_build_tag_name(config, platform_name)
    make_docker_image(tag_name, dockerfile, print_only)

def make_release_image(config, platform_name, context_dir):
    if is_platform_service(config, platform_name):
        return
    print_only = False
    build_tag_name = get_build_tag_name(config, platform_name)
    release_tag_name = get_release_tag_name(config, platform_name)
    dockerfile = make_release_dockerfile(config, platform_name,
                                         build_tag_name)
    if is_cluster_mode(config):
        machine_name = get_master_machine_name(config)
        make_docker_image_remote(machine_name, release_tag_name, dockerfile,
                                 context_dir) 
    else:
        make_docker_image(release_tag_name, dockerfile, print_only, context_dir)
    return release_tag_name

def make_release_dockerfile(config, platform_name, build_image):
    base_image = "ubuntu:bionic" # TODO use config or something
    
    return """
FROM {base_image}
    COPY ./ /opt/install
    WORKDIR /opt/install
""".format(base_image = base_image)

def push_release_image(config, platform_name, tag_name):
    image_name = get_release_tag_name(config, platform_name)
    subprocess.check_output(['docker', 'tag', image_name, tag_name])
    subprocess.check_output(['docker', 'push', tag_name])

def make_service_image_fn(config, platform_name):
    service_type = get_service_type(config, platform_name)
    fn = ServiceFn(nginx_proxy  = make_nginx_proxy_image,
                   postgres     = noop)
    return getattr(fn, service_type)

def get_service_image_tag_name(config, platform_name):
    # this gets the image name to push to registry
    # or None if not needed
    fn = make_service_image_fn(config, platform_name)
    if fn is noop:
        return None
    return get_image_tag_name(config, platform_name, 'service')

def command_gen_certs(args):
    config = load_config_file_with_args(args)
    if args.platform:
        domain = get_domain(config, args.platform)
    else:
        domain = config['cppdock'].get('domain')

    assert domain != None, "Config file must specify domain name"
    import_ssl_certificate(*generate_self_signed_ssl_certificate(domain))

def make_nginx_proxy_image(config, platform_name, print_only):
    tag_name = get_image_tag_name(config, platform_name, 'service')
    conf_data = get_nginx_proxy_conf(config, platform_name)
    dockerfile = """
    FROM nginx:alpine
        RUN echo $'{conf_data}' > /etc/nginx/nginx.conf
    """.format(conf_data = conf_data.replace('\n', '\\n\\\n'))

    make_docker_image(tag_name, dockerfile, print_only)

def command_dev(args):
    config = load_config_file_with_args(args)
    set_master_machine(config)
    platform_name = args.platform
    ports_arg = ' -p=' + args.port if hasattr(args, 'port') and args.port else None
    print_only = False
    make_build_image(config, platform_name, print_only)
    cluster_sync_source(config)
    run_build_image(config, platform_name, args.target, args.shell,
                    args.shell_only, args.privileged, ports_arg)

def command_dev_cluster(args):
    config      = load_config_file_with_args(args)
    set_master_machine(config)

    if (args.stop or args.clean):
        dev_cluster_stop(config)
        if (args.clean):
            dev_cluster_clean_containers(config)
            dev_cluster_clean_volumes(config)
    elif (args.print_only):
        print(get_dev_compose_file(config))
    else:
        # make the build images
        [make_build_image(config, p) for p in get_all_platforms(config)]

        if is_cluster_mode(config):
            print("Running in CLUSTER mode...")
        else:
            print("Running in LOCAL mode...")

        # tags, and pushes the dev images which should be to the dev_registry
        for platform_name in get_all_platforms(config):
            if is_platform_service(config, platform_name):
                tag_name = get_service_image_tag_name(config, platform_name)
                if tag_name:
                    push_docker_image(config, tag_name)
            else:
                tag_name = get_build_tag_name(config, platform_name)
                push_docker_image(config, tag_name)

        cluster_sync_source(config)

        stack_name  = get_stack_name(config)
        run_stack_deploy(config, stack_name)

        print("Finished deploying {stack_name}\n".format(stack_name = stack_name))
        dev_service_name = subprocess.check_output(['docker', 'ps', '-q', '-f name=servicename'])

def command_dev_registry(args):
    config      = load_config_file_with_args(args)
    set_master_machine(config)

    if args.stop:
        dev_registry_rm()
    else:
        dev_registry_create()

def command_install_dep(args):
    # This called inside a container so config is not allowed
    # Even if we add a merged config file to the container we
    # do not want it available at this stage or things will
    # be rebuilt for every config change
    check_sysroot()
    url = make_archive_url(args.repo, args.branch)
    input = urlopen(url)
    output = open('dep.zip', 'wb')
    buf = ''
    while True:
        buf = input.read(800)
        output.write(buf)
        output.flush()
        if (len(buf) == 0):
            break
    output.close()

    assert os.path.isfile('dep.zip')
    assert zipfile.is_zipfile('dep.zip')

    zipfile.ZipFile('dep.zip', 'r').extractall()
    recipe = find_recipe(args.platform, args.repo)
    cwd_dir = [x for x in os.listdir('.') if x.endswith(args.branch)][0]
    p = subprocess.Popen([recipe], cwd = cwd_dir)
    p.communicate('')
    if p.returncode != 0:
        print("""

    FAILURE: Recipe returned error code {0}.
        """.format(p.returncode))
        sys.exit(1)

def command_install_src(args):
    url = make_archive_url(args.repo, args.branch)
    input = urlopen(url)
    output = open('dep.zip', 'wb')
    buf = ''
    while True:
        buf = input.read(800)
        output.write(buf)
        output.flush()
        if (len(buf) == 0):
            break
    output.close()

    assert os.path.isfile('dep.zip')
    assert zipfile.is_zipfile('dep.zip')

    zipfile.ZipFile('dep.zip', 'r').extractall()
    base_src_dir = [x for x in os.listdir('.') if x.endswith(args.branch)][0]
    src_dir = os.path.join(base_src_dir, args.src_dir);
    shutil.copytree(src_dir, args.dest_dir)
    shutil.rmtree(base_src_dir);

def command_init_dev_service(args):
    # command run in container so config is not allowed
    check_sysroot()
    p = run_dev_service_target(args.build_type, args.target,
                               args.shell_only, args.shell,
                               args.keep_alive)
    if p:
        sys.exit(p.returncode)

def command_cluster_build(args):
    config = load_config_file_with_args(args)
    machine_create_all(config)
    cluster_sync_source_init(config)
    cluster_sync_source(config)

def command_cluster_sync(args):
    config = load_config_file_with_args(args)
    cluster_sync_source(config)

def command_cluster_start(args):
    config = load_config_file_with_args(args)
    machine_start_all(config)

def command_cluster_stop(args):
    config = load_config_file_with_args(args)
    machine_stop_all(config)

def command_cluster_rm(args):
    config = load_config_file_with_args(args)
    machine_rm_all(config)

def command_clean(args):
    # Use `cppdock dev_cluster --stop` for deployed stacks.
    config = load_config_file_with_args(args)
    set_master_machine(config)
    platforms = [args.platform] if args.platform else get_all_platforms(config)
    clean_types = [args.clean_type] if args.clean_type != 'all' else ['install',
                                                                      'build',
                                                                      'images']
    for clean_type in clean_types:
        for platform_name in platforms:
            if clean_type == 'install':
                # remove the host machine install directory
                install_dir = get_install_path(config, platform_name)
                clean_install_files(config, platform_name, install_dir)
                remove_install_dir(config, install_dir)
            elif clean_type == 'build':
                # remove the build volume
                subprocess.check_output(['docker', 'volume', 'rm', '--force',
                        get_volume_name(config, platform_name, 'release'),
                        get_volume_name(config, platform_name, 'debug')])
            elif clean_type == 'images':
                # remove all build and release images
                tag_names = [get_build_tag_name(config, platform_name),
                             get_base_tag_name(config, platform_name),
                             get_release_tag_name(config, platform_name)]
                subprocess.check_output(['docker', 'image', 'rm', '--force',
                                         get_build_tag_name(config, platform_name),
                                         get_base_tag_name(config, platform_name),
                                         get_release_tag_name(config, platform_name)])

            else: 
                raise ValueError('Unsupported clean type')

def install_deps_platform(platform, items):
    for i in range(len(items)):
        repo, branch = items[i]
        branch = branch[0:40]
        install_dep(i, platform, repo, branch)

def load_config_file_with_args(args):
    json_file_path = args.json_file_path
    return load_config_file(json_file_path)

def load_config_file(json_file_path):
    config_paths = []
    config_paths.insert(0, json_file_path)
    config = load_config_file_json(json_file_path)

    config = lock_config_file_deps(config)
    write_config_file(config, json_file_path)

    config = normalize_config_paths(config)
    config = merge_base_config(config, config_paths)
    config = process_config_replace_deps(config)

    source = config.get('source', {})
    config['source'] = source
    source['allow_sync'] = calculate_allow_sync(config)
    source["path"] = calculate_source_path(config, config_paths)

    cppdock = config['cppdock']
    cppdock["cppdock_image"] = cppdock.get("cppdock_image",
                                           "ricejasonf/cppdock")
    config["machine_list"] = get_machine_list(config)
    return config

def load_config_file_json(json_file_path):
    if os.path.isdir(json_file_path):
        json_file_path = os.path.join(json_file_path, 'cppdock.json')

    if not os.path.isfile(json_file_path):
        raise ValueError('Config file not found: ' + json_file_path)
    stream = open(json_file_path, 'r')
    config = json.load(stream, object_pairs_hook=OrderedDict)

    # validate
    assert "cppdock" in config,             "Config must have cppdock section"
    assert "name" in config["cppdock"],     "Config must have 'name' in cppdock section"

    return config

def normalize_config_paths(config):
    # this should happen after writing revision locks
    cppdock = config["cppdock"]
    source = config.get("source")
    if "base_config" in cppdock:
        cppdock["base_config"] = os.path.abspath(cppdock["base_config"])
    if source and "path" in source and source["path"] != None:
        source["path"] = os.path.abspath(source["path"])

    return config

def write_config_file(config, json_file_path):
    try:
        with open(json_file_path, 'w') as fh:
            json.dump(config, fh, indent=2, separators=(',', ': '))
            fh.write('\n')
    except:
        warnings.warn('WARNING: Unable to write to json file')

def merge_base_config(config, config_paths):
    if "base_config" not in config["cppdock"]:
        return config
    base_config_path = os.path.abspath(config["cppdock"]["base_config"])
    del config["cppdock"]["base_config"]
    config_paths.insert(0, base_config_path)
    base_config = load_config_file_json(base_config_path)

    # merge cppdock section
    cppdock_section = {}
    cppdock_section.update(base_config["cppdock"])
    cppdock_section.update(config["cppdock"])
    config["cppdock"] = cppdock_section

    # only include platforms specified in current config
    platforms = {}

    for platform_name in config.get("platforms", {}):
        platforms[platform_name] = merge_platform_config(platform_name, base_config, config)

    config["platforms"] = platforms
    return config

def merge_platform_config(platform_name, base_config, config):
    result = {}
    platform = config["platforms"].get(platform_name, {})
    base_platform = base_config.get("platforms", {}).get(platform_name, {})

    result.update(base_platform)
    result.update(platform)
    if config.get('deps') or base_config.get('deps'):
        result["deps"] = []
        result["deps"] += base_platform.get("deps", [])
        result["deps"] += platform.get("deps", [])
    return result

def process_config_replace_deps(config):
    # TODO read "replace_deps" to replace all deps with a
    #      specified name or a dep with a specific id
    #      not sure if this is desirable yet
    return config

def lock_config_file_deps(config):
    if 'platforms' not in config:
        return config
    for platform, settings in config['platforms'].items():
        check_base_image_conflict(settings)

        for stage in settings.get('deps', []):
            if not isinstance(stage, list):
                raise "Each stage in 'deps' must be an array."
            for item in stage:
                item = lock_item_json(item)

    return config

def lock_item_json(item):
    current_sha = get_current_sha_with_comment(item)
    item.update(revision = current_sha)

    return item

def get_current_sha_with_comment(item):
    repo = item['name']
    tag = item.get('tag')
    source = item.get('source')

    if 'revision' in item:
        return item['revision']
    else:
        if source == 'docker':
            tag = tag or 'latest'
            repo_tag = repo + ":" + tag

            try:
                # pull the image from the registry to get the RepoDigest
                subprocess.check_output(['docker', 'pull', repo_tag])
            except:
                raise RuntimeError("Unable to pull docker image from registry: " + repo_tag )

            try:
                inspect_result =  subprocess.check_output(['docker', 'inspect', repo_tag])
                digest = json.loads(inspect_result)[0]['RepoDigests'][0].split('@')[1]
            except:
                raise RuntimeError("Unable to get SHA from docker image")

            return digest
        else:
            tag = normalize_branch_name(tag)
            lines = subprocess.check_output(['git', 'ls-remote', normalize_repo_name(repo)]).splitlines()
            for line in lines:
                if line.endswith(tag):
                    return line[0:40]
            raise RuntimeError("Unable to get SHA from remote git repository")

def normalize_repo_name(name):
    return default_dep_path + name;

def normalize_branch_name(name):
    if name == None or len(name) == 0 or name == 'HEAD':
        return 'HEAD'
    elif name.startswith('refs/'):
        return name
    else:
        return "refs/heads/" + name

def is_sha(name):
    return len(name) == 40

def check_base_image_conflict(settings):
    if 'base_image' in settings and 'base_image_dockerfile' in settings:
        raise StandardError("Conflicting base image settings detected")

def get_max_key_length(config):
    length = 0
    for section in config.sections():
        for item in config.items(section):
            len_ = len(item[0])
            if (len_ > length):
                length = len_
    return length

def make_archive_url(repo, branch):
    return 'https://github.com/{0}/archive/{1}.zip'.format(repo, branch)

def get_base_image_dockerfile(config, platform_name):
    return config['platforms'][platform_name].get('base_image_dockerfile')

def get_user_base_tag_name(config, platform_name):
    if get_base_image_dockerfile(config, platform_name):
        return get_image_tag_name(config, platform_name, 'user_base')

    tag_name =  config['platforms'][platform_name].get('base_image')
    if tag_name:
        return tag_name

    platform_type = get_platform_type(config, platform_name)
    return get_config_option(config, 'platform_' + platform_type)

def get_base_image_dockerfile_data(config, platform_name):
    tag_name = get_user_base_tag_name(config, platform_name)
    cppdock_tag_name = config['cppdock']['cppdock_image']

    return """
FROM {tag_name}
    # installs cppdock in base image
    COPY --from={cppdock_tag_name} /opt/install/ /usr/local
    COPY --from={cppdock_tag_name} /root/.cppdock_recipes/ /root/.cppdock_recipes
    RUN mkdir -p /opt/install && \
        mkdir -p /opt/build && \
        touch /opt/toolchain.cmake
    """.format(tag_name = tag_name,
               cppdock_tag_name = cppdock_tag_name)

def make_base_image(config, platform_name, print_only):
    make_base_image_dockerfile(config, platform_name, print_only)
    dockerfile = get_base_image_dockerfile_data(config, platform_name)
    tag_name = get_base_tag_name(config, platform_name)
    make_docker_image(tag_name, dockerfile, print_only)

def make_base_image_dockerfile(config, platform_name, print_only):
    base_image_dockerfile = get_base_image_dockerfile(config, platform_name)

    if base_image_dockerfile:
        dockerfile = "./{0}".format(base_image_dockerfile)
        tag_name = get_user_base_tag_name(config, platform_name)

        with open(dockerfile, 'r') as file:
            dockerfile_data = file.read()
        make_docker_image(tag_name, dockerfile_data, print_only)

def find_recipe(platform, repo):
    # make a flat list of recipe file paths
    # to search linearly
    base_paths = ['/opt/cppdock_recipes',
                  '/opt/sysroot/cppdock_recipes',
                  '/root/.cppdock_recipes']

    repo = repo.replace('/', '-')
    repo_with_platform = repo + '-' + platform
    default_with_platform = 'default' + '-' + platform
    file_names = [repo_with_platform,
                  repo,
                  default_with_platform,
                  'default']

    product = itertools.product(base_paths, file_names)
    xs = [os.path.join(a, b) for (a, b) in product]
    for x in xs:
        if is_verbose_mode():
            print("Trying Recipe Location: {0}".format(x))
        if os.path.isfile(x):
            return x
    raise ValueError('Unable to find cppdock recipe: ' + repo_with_platform)

def check_sysroot():
    # This should only be called inside a build container
    if not os.path.exists('/opt/sysroot'):
        # implicitly create /opt/sysroot if none exists
        os.symlink('/usr/local', '/opt/sysroot')
        assert os.path.isdir('/opt/sysroot/')

def get_arg(args, i):
    next(iter(args[i:i+1]), None)

def get_config_option(config, name):
    defaults = {
        'project': None,
        'platform_linux_x64':     'ricejasonf/cppdock:linux_x64',
        'platform_emscripten':    'ricejasonf/cppdock:emscripten',
        'platform_tvossimulator': 'ricejasonf/cppdock:tvossimulator',
    }
    value = None

    if name in defaults:
        value = defaults[name]
    if value == None:
        raise ValueError('Config option has no default for "{0}"'.format(name))
    return value

def make_docker_image(tag_name, dockerfile_data, print_only = False, context_dir = '.'):
    if print_only:
        print('###### Docker Image: ' + tag_name + ' ######')
        print(dockerfile_data)
        return
    p = subprocess.Popen(['docker', 'buildx', 'build', '--tag=' + tag_name,
                          '--file=-', context_dir], stdin=subprocess.PIPE)
    out, err = p.communicate(bytes(dockerfile_data, 'utf-8'))
    if p.returncode == 0:
        print("Finished building {0}.".format(tag_name))
    else:
        print("""

    FAILURE: Build of {0} FAILED.

        """.format(tag_name))
        sys.exit(1)

    return (p.returncode, out, err)

def make_docker_image_remote(machine_name, tag_name, dockerfile_data, context_dir):
    # runs docker build on specifed machine
    cmd = ['docker', 'buildx', 'build', '--tag=' + tag_name, '--file=-',
           context_dir]
    cmd = ['docker-machine', 'ssh', machine_name, 'sudo'] + cmd
    p = subprocess.Popen(cmd, stdin=subprocess.PIPE)
    out, err = p.communicate(bytes(dockerfile_data, 'utf-8'))
    if p.returncode == 0:
        print("Finished building {0}.".format(tag_name))
    else:
        print("""

    FAILURE: Build of {0} FAILED.

        """.format(tag_name))
        sys.exit(1)

    return (p.returncode, out, err)

def push_docker_image(config, tag_name):
    # tags with registry and pushes
    registry = get_registry(config)
    rtag_name = registry + '/' + tag_name
    print("Pushing {}".format(rtag_name))
    subprocess.check_output(['docker', 'tag', tag_name, rtag_name])
    subprocess.check_output(['docker', 'push', rtag_name])
    print("Finished pushing {0}".format(rtag_name))
    return rtag_name

def get_platform_type(config, platform_name):
    try:
        return config['platforms'][platform_name]['type']
    except:
        raise ValueError('Platform name `{0}` not found.'.format(platform_name))

# is_platform_service(config, platform_name)
# is_platform_service(platform_type)
def is_platform_service(*argv):
    if len(argv) == 1:
        [platform_type] = argv
    elif len(argv) == 2:
        [config, platform_name] = argv
        platform_type = get_platform_type(config, platform_name)
    return platform_type == 'service' or platform_type.startswith('service:')

def get_project_name(config):
    return config['cppdock']['name']

def make_deps_dockerfile(config, platform_name):
    platform_type = get_platform_type(config, platform_name)
    # move this to be used when building the base image
    #base_image = get_base_image(config, platform_name) or get_config_option(config, 'platform_' + platform_type)
    base_image = get_base_tag_name(config, platform_name)
    deps = get_deps_from_config(config, platform_name)
    deps_stages = ''
    deps_imports = ''

    build_type = get_build_type(config, platform_name)
    target = get_dev_service_target(config, platform_name)

    dev_service_init = get_dev_service_init(config, platform_name)
    copy_dev_service_init = ''
    if dev_service_init:
        copy_dev_service_init = "COPY {} /opt/dev_service_init".format(
                dev_service_init)

    for dep in deps:
        deps_stages += " ".join([make_deps_stage(base_image, platform_name, dep)])
        deps_imports += make_deps_import(platform_name, dep[-1][2])

    return """
{deps_stages}
FROM {base_image}
{deps_imports}
    {copy_dev_service_init}
    WORKDIR /opt/build
    CMD cppdock init_dev_service --shell-only {build_type} {target}
""".format(deps_stages              = deps_stages,
           base_image               = base_image,
           deps_imports             = deps_imports,
           copy_dev_service_init    = copy_dev_service_init,
           build_type               = build_type,
           target                   = target)
    return

def make_deps_stage(base_image, platform_name, deps):
    copy_recipes_term = ""
    build_image = base_image

    # only one dep that is source: docker does not use make_install_dep

    docker_image_digest = get_stage_docker_image_digest(deps)
    if docker_image_digest is not None:
      build_image  = docker_image_digest
      install_deps = ''
    else:
      install_deps = " ".join([make_install_dep(x, y, z, zz) for x, y, z, zz in deps])

    revision = deps[-1][2].replace('/', '_')

    return """
FROM {0} as build_{2}_{1}
    {4}
    WORKDIR /usr/local/src
    {3}
""".format(build_image, platform_name, revision, install_deps, copy_recipes_term)

def get_stage_docker_image_digest(stage_deps):
    if len(stage_deps) == 1:
        for platform, revision, repo, source in stage_deps:
            if source == 'docker':
                # repo@revision
                return '{0}@{1}'.format(repo, revision)
    return None


def make_install_dep(platform, revision, repo, source=''):
    if source == 'docker':
        return """
    COPY --from={0}@{1} /opt/install /opt/sysroot
    COPY --from={0}@{1} /opt/install /opt/install""".format(repo, revision)

    if not is_sha(revision):
        return ''
    repo_recipe_prefix = repo.replace('/', '-')
    copy_local_recipe = ''
    if os.path.isfile(os.path.join(root_path, 'cppdock_recipes', repo_recipe_prefix + '-' + platform)):
        copy_local_recipe = """
    COPY cppdock_recipes/{0}-{1} /opt/cppdock_recipes/""".format(repo_recipe_prefix, platform)
    elif os.path.isfile(os.path.join(root_path, 'cppdock_recipes', repo_recipe_prefix)):
        copy_local_recipe = """
    COPY cppdock_recipes/{0} /opt/cppdock_recipes/""".format(repo_recipe_prefix)

    return """{3}
    RUN cppdock install_dep {0} {1} {2}""".format(platform, revision, repo, copy_local_recipe)

def make_deps_import(platform, revision):
    return """
    COPY --from=build_{1}_{0} /opt/install/ /opt/sysroot""".format(platform, revision.replace('/', '_'))

def get_deps_from_config(config, platform_name):
    platform = config['platforms'][platform_name]

    return [get_deps_stage_items(stage, platform['type']) \
        for stage in platform.get('deps', [])]

    raise ValueError('Platform is not specified in cppdock json file')

def get_deps_stage_items(deps_stage, platform_type):
    return [(platform_type, item['revision'],  item['name'], item.get('source')) for item in deps_stage]

def get_run_options_mount_source(config):
    src_path = get_build_source_path(config)
    return ['--mount', 'type=bind,source=' + src_path + ',target=/opt/src,readonly']

def get_run_options_mount_build(config, platform_name):
    volume_name = get_volume_name(config, platform_name)
    return ['--volume', '{}:/opt/build'.format(volume_name)]

def get_run_options_mount_install(config, platform_name, install_dir):
    return ['--mount', 'type=bind,source=' + install_dir + ',target=/opt/install']

def run_build_image(config, platform_name, target, shell, shell_only, privileged, ports_arg):
    tag_name = get_build_tag_name(config, platform_name)

    cmd = []
    shell_op = '--shell' if shell else None;
    shell_only_op = '--shell-only' if shell_only else None;
    privileged_op = '--privileged' if privileged else None;
    if target:
        build_type = get_build_type(config, platform_name)
        cmd = ["cppdock", "init_dev_service", build_type, target, shell_op,
               shell_only_op, '--no-service']
    mount_build = get_run_options_mount_build(config, platform_name)
    mount_src = get_run_options_mount_source(config)

    subprocess.call(remove_none(['docker', 'run', '--rm', '-it', ports_arg, privileged_op]
                                 + mount_src 
                                 + mount_build
                                 + [ tag_name ]
                                 + cmd))

def run_build_image_install(config, platform_name, install_dir):
    tag_name = get_build_tag_name(config, platform_name)
    src_path = get_build_source_path(config)
    target = get_release_target(config, platform_name)
    build_type = get_build_type(config, platform_name)

    cmd = ["cppdock", "init_dev_service", build_type, target, '--no-service']

    mount_build = get_run_options_mount_build(config, platform_name)
    mount_src = get_run_options_mount_source(config)
    mount_install = get_run_options_mount_install(config, platform_name, install_dir)
    returncode = subprocess.call(remove_none(['docker', 'run', '--rm']
                                 + mount_src 
                                 + mount_build
                                 + mount_install
                                 + [ tag_name ]
                                 + cmd))
    if returncode != 0:
        sys.exit(1)

def get_install_path(config, platform):
    return '/tmp/{}.install'.format(get_release_tag_name(config, platform))

def create_install_dir(config, platform):
    path = get_install_path(config, platform)
    cmd = ['mkdir', '-p', path]

    if is_cluster_mode(config):
        name = get_master_machine_name(config)
        cmd = ['docker-machine', 'ssh', name] + cmd

    p = subprocess.Popen(cmd, stdin = subprocess.PIPE)
    p.communicate()

    if p.returncode != 0:
        raise RuntimeError(
            'Failed to create install directory: {}'.format(path))
    return path

def remove_install_dir(config, path):
    assert path.startswith('/tmp/') and path.endswith('.install'), "Must only delete valid install path"
    if not os.path.isdir(path):
        return
    cmd = ['rmdir', path]

    if is_cluster_mode(config):
        name = get_master_machine_name(config)
        cmd = ['docker-machine', 'ssh', name] + cmd

    p = subprocess.Popen(cmd, stdin = subprocess.PIPE)
    p.communicate()

    if p.returncode != 0:
        raise RuntimeError(
            'Failed to remove install directory: {}'.format(path))
    return path

def clean_install_files(config, platform_name, install_dir):
    # We have to delete the installs files in a container
    # because the cppdock user wont have to permissions
    if not os.path.isdir(install_dir):
        return
    base_image = config['cppdock']['cppdock_image']
    assert install_dir.startswith('/tmp/') and install_dir.endswith('.install'), "Must only delete valid install path"
    cmd = ['/bin/bash', '-c', 'rm -rf /opt/install/*']
    mount_install = get_run_options_mount_install(config, platform_name, install_dir)
    cmd = remove_none(['docker', 'run', '--rm']
                      + mount_install
                      + [ base_image ]
                      + cmd)
    subprocess.call(cmd)

def run_stack_deploy(config, stack_name):
    # docker stack deploy is already idempotent
    compose_file_data = get_dev_compose_file(config)
    p = subprocess.Popen(['docker', 'stack', 'deploy', '--prune', '--compose-file', '-', stack_name], stdin=subprocess.PIPE)
    p.communicate(input=bytes(compose_file_data, 'utf-8'))
    if p.returncode != 0:
        print("""

    FAILURE: `docker stack deploy` for `{stack_name}` error code {error_code}.
        """.format(stack_name=stack_name, error_code=p.returncode))
        sys.exit(1)

def run_stack_service_terminal(config, stack_name, platform_name):
    service_name = get_service_name(config, platform_name)
    print("""
    TODO shell into {service_name} somehow
    """)
    # container_id = # get_container_id(config, platform_name)
    # TODO finish
    #os.system("docker exec -it {0} bash".format(container_id)

def get_dev_compose_file(config):
    result = get_dev_compose_object(config)
    return json.dumps(remove_none(result), indent=2, separators=(',', ': '))

def get_dev_compose_object(config):
    platform_names = get_all_platforms(config)
    service_confs = [get_dev_compose_file_service(config, platform_name) for platform_name in platform_names]
    secrets = config.get('secrets', {})
    secrets.update(get_secret_files());

    services = {}
    volumes = {}
    for x in service_confs:
        services.update(x['service'])
        secrets.update(x.get('secrets', {}))
        volumes.update(x.get('volumes', {}))

    return {
        'version': '3.1',
        'services': services,
        'volumes': volumes,
        'secrets': secrets
    }

def get_dev_compose_file_service(config, platform_name):
    platform = get_platform_config(config, platform_name)
    if is_platform_service(config, platform_name):
        return get_dev_compose_file_service_service(config, platform_name)

    # generate service definition for dev images

    service_name = get_service_name(config, platform_name)
    image_tag = get_build_tag_name(config, platform_name)
    image_tag = get_registry_tag(config, image_tag)
    target = get_dev_service_target(config, platform_name)
    build_type = get_build_type(config, platform_name)

    command = "cppdock init_dev_service {build_type} {target}".format(
            build_type = build_type,
            target = target)

    web_root = platform.get('web_root')
    if web_root:
        web_root = "web_root:{0}".format(web_root)

    src_mount_path = get_build_source_path(config)
    src_mount = '{src_mount_path}:/opt/src:ro'.format(src_mount_path = src_mount_path)
    domain = get_domain(config, platform_name)
    environment = { 'DOMAIN': domain }
    environment.update(platform.get('environment', {}))
    secrets = platform.get('secrets', [])
    build_volume_name = platform_name + '_build'

    return {
        'service': {
            service_name: {
                'image': image_tag,
                'command': command,
                'environment': environment,
                'secrets': secrets,
                'volumes': [
                    src_mount,
                    '{}:/opt/build'.format(build_volume_name),
                    web_root
                ]
            }
        },
        'volumes': {
            build_volume_name: None
        }
    }

def get_dev_compose_file_service_service(config, platform_name):
    service_type = get_service_type(config, platform_name)
    if (service_type == None):
        default_compose_file_service(config, platform_name)

    fn = ServiceFn(nginx_proxy = get_service_nginx_proxy,
                   postgres    = get_service_postgres)
    return getattr(fn, service_type)(config, platform_name)

def default_compose_file_service(config, platform_name):
    result = {}
    platform = get_platform_config(config, platform_name)
    result.update(platform)
    del result['type']
    return {
        'service': {
            platform_name: result
        }
    }

def get_volume_name(config, platform_name, build_type = None):
    if build_type == None:
        build_type = get_build_type(config, platform_name).lower()
    return get_image_tag_name(config, platform_name, build_type)

def get_build_tag_name(config, platform_name):
    return get_image_tag_name(config, platform_name, "build")

def get_release_tag_name(config, platform_name):
    return get_image_tag_name(config, platform_name, "release")

def get_base_tag_name(config, platform_name):
    return get_image_tag_name(config, platform_name, 'base')

def get_image_tag_name(config, platform_name, image_type):
    return "{project}-{platform}{image_type}".format(
        project     = get_project_name(config),
        platform    = platform_name,
        image_type  = '-' + image_type if image_type != 'release' else '',
    )

def get_stack_name(config):
    return get_project_name(config)

def get_all_platforms(config):
    return config['platforms'].keys()

def get_platform_config(config, platform_name):
    return config['platforms'][platform_name]

def get_dev_service_target(config, platform_name):
    platform = get_platform_config(config, platform_name)
    service_target = platform.get('service_target')
    return platform.get('target') or service_target or 'check'

def get_dev_service_init(config, platform_name):
    base_path = os.path.join('.', 'cppdock_recipes')
    path1 = os.path.join(base_path, 'this-' + platform_name)
    path2 = os.path.join(base_path, 'this')
    if os.path.isfile(path1):
        return path1
    elif os.path.isfile(path2):
        return path2
    else:
        return None

def run_dev_service_cmake(build_type, target = None, init_only = ''):
    cmake_commands = ['cmake']
    emscripten_root = os.getenv('EMSCRIPTEN_ROOT')
    if emscripten_root:
        emcmake = os.path.join(emscripten_root, 'emcmake')
        cmake_commands = [emcmake, 'cmake']
    p = None
    if not os.path.isfile('/opt/build/CMakeCache.txt'):
        p = subprocess.Popen(cmake_commands + [
                              '-DCMAKE_TOOLCHAIN_FILE=/opt/toolchain.cmake',
                              '-DCMAKE_INSTALL_PREFIX=/opt/install',
                              '-DCMAKE_BUILD_TYPE=' + build_type,
                              '/opt/src'],
                             cwd = '/opt/build')
        p.communicate('')

    if (p == None or p.returncode == 0) and target and not init_only:
        p = subprocess.Popen(['cmake', '--build', '.', '--target', target],
                             cwd = '/opt/build')
        p.communicate('')
        return p

    return None

def run_dev_service_init(build_type, target, init_only = ''):
    cmd = remove_none(['/opt/dev_service_init'])
    env = dict(os.environ, BUILD_TYPE   = build_type,
                           TARGET       = target,
                           INIT_ONLY    = init_only)
                           
    p = subprocess.Popen(cmd, cwd = '/opt/build', env = env)
    p.communicate('')
    return p

def run_dev_service_target(build_type, target,
                           shell_only = False,
                           shell      = False,
                           keep_alive = True):
    # This should run in a docker container

    has_init= os.path.isfile('/opt/dev_service_init')
    init_fn = run_dev_service_init if has_init else run_dev_service_cmake

    if shell_only:
        init_fn(build_type, '', "INIT_ONLY")
        p = subprocess.Popen(['/bin/bash'])
        p.communicate('')
        return p

    p = init_fn(build_type, target)

    if not keep_alive:
        return p

    # Make the service target and just hang out
    # Things will show up in the logs and the user can still shell in to inspect things

    if p and p.returncode != 0:
        print("Service exited with error {}.".format(p.returncode))

    # hang out so the container stays alive

    if shell:
        p = subprocess.Popen(['/bin/bash'])
        p.communicate('')
    else:
        p = subprocess.Popen(['tail', '-f', '/dev/null'])
        p.communicate('')
    return p

def cluster_get_worker_token():
    try:
        return subprocess.check_output(['docker', 'swarm', 'join-token', '-q', 'worker'])
    except:
        raise RuntimeError("Unable to get cluster worker token")

def cluster_get_sync_path(path):
    return "/home/ubuntu/sync{}".format(path)

def cluster_sync_source(config):
    if not is_allowed_cluster_sync(config):
        print('Skipping source code sync')
        return
    machines = get_machine_list(config)
    machine_names = [x[0] for x in machines]
    for name in machine_names:
        cluster_sync_source_rsync_node(config, name)

# Intializes source code on all machines
# after they are built.
def cluster_sync_source_init(config):
    machines = get_machine_list(config)
    machine_names = [x[0] for x in machines]

    script_data = ''
    source_init = config['source'].get('init')
    source_repo = config['source'].get('repo')
    if source_init:
        with open(source_init, 'r') as fh:
            script_data += fh.read()
    elif source_repo:
        source_repo_tag = config['source'].get('tag', 'master')
        script_data = "git clone --depth 1 --branch {tag} --single-branch {repo} .".format(
                repo = source_repo,
                tag  = source_repo_tag)
    else:
        script_data = ''

    for name in machine_names:
        cluster_sync_source_init_node(config, name, script_data)

def cluster_sync_source_init_node(config, name, script_data):
    path = get_build_source_path(config)
    print('Initializing source code on {}'.format(name))
    p = subprocess.Popen(['docker-machine', 'ssh', name, 'bash', '-s'],
                         stdin = subprocess.PIPE)
    script_header = 'mkdir -p {path} && cd {path};'
    p.communicate(bytes(script_header.format(path = path) + script_data, 'utf-8'))
    if p.returncode != 0:
        raise RuntimeError(
            'Failed to initialize source code on {name}'.format(
                name = name))

def cluster_sync_source_rsync_node(config, name):
    assert is_allowed_cluster_sync(config), "Must not attempt to rsync with nothing"
    base_src_path = config['source']['path']
    assert base_src_path, 'Config source.path must be set'
    base_sync_path = get_build_source_path(config)
    def sync_dir(path):
        src_path = os.path.join(base_src_path, path, '.')
        dest_path = os.path.join(base_sync_path, path)
        machine_rsync(name, src_path, dest_path)

    [sync_dir(x) for x in config['cppdock'].get('sync_dirs', [''])]

def get_nginx_proxy_conf(config, platform_name):
    domain = get_domain(config, platform_name)
    assert domain, 'Nginx proxy requires a domain name'
    locations           = ''
    upstreams           = ''
    redirect_servers    = ''

    if domain.startswith('www.'):
        non_www_domain = domain[4:]
        redirect_servers += """
    server {{
        # redirect to https
  server_name {domain} {non_www_domain};
  return 301 https://{domain}$request_uri;
    }}
    server {{
        # redirect to www.
        listen 443 ssl;
        server_name {non_www_domain};
        ssl_certificate /run/secrets/{domain}.crt;
        ssl_certificate_key /run/secrets/{domain}.key;
  return 301 https://{domain}$request_uri;
    }}""".format(domain         = domain,
                 non_www_domain = non_www_domain)
    else:
        redirect_servers += """
    server {{
        # redirect to https
  server_name {domain};
  return 301 https://{domain}$request_uri;
    }}""".format(domain = domain)

    for platform_name in get_all_platforms(config):
        if is_platform_service(config, platform_name):
            continue;
        locations += get_nginx_proxy_conf_locations(config, platform_name)
        upstreams += get_nginx_proxy_conf_upstreams(config, platform_name)
    return """
user  nginx;
worker_processes  1;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;

events {{
  worker_connections  1024;
}}

http {{
    include       /etc/nginx/mime.types;
    types {{
        application/wasm wasm;
        font/woff2 woff2;
    }}
    default_type  application/octet-stream;
    resolver 127.0.0.11;

    log_format  main  \\'$remote_addr - $remote_user [$time_local] "$request" \\'
          \\'$status $body_bytes_sent "$http_referer" \\'
          \\'"$http_user_agent" "$http_x_forwarded_for"\\';

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;
    #tcp_nopush     on;

    keepalive_timeout  65;

    #gzip  on;

    {upstreams}

    {redirect_servers}

    server {{
        listen 443 ssl;
        server_name {domain};
        ssl_certificate   /run/secrets/{domain}.crt;
        ssl_certificate_key   /run/secrets/{domain}.key;
        root /opt/web_root;
        {locations}
        location / {{
            # web app uri redirect
            try_files $uri $uri/ /index.html;
        }}
    }}
}}
    """.format(locations        = locations,
               upstreams        = upstreams,
               domain           = domain,
               redirect_servers = redirect_servers)

def get_nginx_proxy_conf_upstreams(config, platform_name):
    platform = config['platforms'][platform_name]
    # upstream directive for each port exposed to the internal network
    # ports: ["{name}:{protocol}:{port}"] |
    #        ["{name}:{port}"] |
    #        ["{port}"]
    #   name        - (optional) specify name to append to service name
    #   prototcol   - (optional) specify ws or http or something else
    #   port        - (required) port number
    upstreams = ''
    port_specs = get_platform_port_specs(config, platform_name)

    for port_name, protocol, port in port_specs:
        service_name = get_service_name(config, platform_name)

        if protocol == 'ws':
            upstreams += """
    upstream docker-{service_name} {{
        server {service_name}:{port};
    }}""".format(service_name  = service_name,
                 port          = port)
        else:
            raise ValueError("Invalid protocol")

    return upstreams

def get_nginx_proxy_conf_locations(config, platform_name):
    platform = config['platforms'][platform_name]
    # location directive for each port exposed to the internal network
    # ports: ["{name}:{protocol}:{port}"] |
    #        ["{name}:{port}"] |
    #        ["{port}"]
    #   name        - (optional) specify name to append to service name
    #   prototcol   - (optional) specify ws or http or something else
    #   port        - (required) port number
    # uri should be uri prefix plus service_name
    locations = ''
    port_specs = get_platform_port_specs(config, platform_name)

    for port_name, protocol, port in port_specs:
        service_name = get_service_name(config, platform_name)

        uri = config.get('uri_prefix') or '/'
        uri += service_name
        if len(port_name) > 0:
            uri += '_' + port_name
        if protocol == 'ws':
            locations += get_nginx_location_websocket(uri, service_name, port)
        else:
            raise ValueError("Invalid protocol")

    return locations

# returns [[name, protocol, port]]
def get_platform_port_specs(config, platform_name):
    platform = config['platforms'][platform_name]
    port_specs = platform.get('ports') or []
    results = []
    for port_spec in port_specs:
        port_data    = port_spec.split(':')
        length       = len(port_data)

        if length == 1:
            [port] = port_data
            port_name = ''
            protocol = 'ws'
        elif length == 2:
            [port_name, port] = port_data
            protocol = 'ws'
        elif length == 3:
            port_name, protocol, port = port_data
        else:
            raise ValueError("Invalid port specification")

        # only websockets are supported
        if protocol not in ['ws']:
            raise ValueError("Invalid protocol specified")

        results.append([port_name, protocol, port])

    return results

def get_nginx_location_websocket(uri, service_name, port):
    # creates a location with an exact uri match of the platform name
    # it proxies to the websocket backend
    return """
      location = {uri} {{
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header Host $http_host;
        proxy_set_header X-NginX-Proxy true;

        proxy_pass http://docker-{service_name};
        proxy_redirect off;

        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
      }}""".format(uri           = uri,
                   service_name  = service_name,
                   port          = port)

def get_service_nginx_proxy(config, platform_name):
    domain = get_domain(config, platform_name)
    assert domain, 'Nginx proxy requires a domain name'
    platform = get_platform_config(config, platform_name)
    service_name_fn = lambda x : get_service_name(config, x)
    service_names = map(service_name_fn, get_all_platforms(config))
    tag_name = get_image_tag_name(config, platform_name, 'service')
    tag_name = get_registry_tag(config, tag_name)
    volume_name = platform.get('volume_name') or 'web_root'
    return {
        'service': {
            platform_name: {
                'image': tag_name,
                'ports': ['80:80', '443:443'],
                'volumes': [
                    '{}:/opt/web_root:ro'.format(volume_name)
                ],
                'secrets': [
                    "{}.crt".format(domain),
                    "{}.key".format(domain)
                ],
                'depends_on': service_names
            }
        },
        'volumes': {
            volume_name: None
        }
    }

def get_service_postgres(config, platform_name):
    volume_name = platform_name
    return {
        'service': {
            platform_name: {
                'image': 'postgres',
                'environment': {
                    'POSTGRES_USER_FILE': '/run/secrets/POSTGRES_USER',
                    'POSTGRES_PASSWORD_FILE': '/run/secrets/POSTGRES_PASSWORD'
                },
                'secrets': [
                    'POSTGRES_USER',
                    'POSTGRES_PASSWORD'
                ],
                'volumes': [
                    '{}:/var/lib/postgresql/data'.format(volume_name)
                ],
                'deploy': {
                    'placement': {
                        'constraints': ['node.role == manager']
                    }
                }
            },
        },
        'volumes': {
            volume_name: None
        }
    }

def is_cluster_mode(config):
    return len(get_machine_list(config)) > 0

def get_service_name(config, platform_name):
    return platform_name

def generate_self_signed_ssl_certificate(domain):
    subj = '/C=US/ST=Nevada/L=Henderson/O=Dis/CN={}'.format(domain)
    try:
      key_data = subprocess.check_output(['openssl', 'genrsa', '4096'])
    except:
      raise RuntimeError('Failed to generate SSL key (via openssl)')

    # Not sure about this if we need it to ever be secure
    print("Creating self signed certificate...")
    p = subprocess.Popen([
        'openssl', 'req',
            '-new', '-x509', '-key', '/dev/stdin',
            '-subj', subj],
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE)
    crt_data, crt_err = p.communicate(bytes(key_data, 'utf-8'))

    if p.returncode != 0:
      raise RuntimeError('Failed to generate SSL cert (via openssl)')

    return (domain, key_data, crt_data)

def import_ssl_certificate(domain, key_data, crt_data):
    # Add to ./cppdock_secrets/
    key_name = '{}.key'.format(domain)
    crt_name = '{}.crt'.format(domain)
    print('Adding certificates to secret database...')
    make_secret_file(key_name, key_data)
    make_secret_file(crt_name, crt_data)

def make_secret_file(name, data):
    path = os.path.join(os.getcwd(), 'cppdock_secrets', name)
    with open(path, 'w') as fh:
        fh.write(data)

def get_secret_files():
    # We never want to look in the source.path for secrets
    # so this is a valid use of getcwd
    secrets_dir = os.path.join(os.getcwd(), 'cppdock_secrets')
    if not os.path.isdir(secrets_dir):
        return {}
    names = [f for f in os.listdir(secrets_dir)]
    secrets = {}
    for name in names:
        path = os.path.abspath(os.path.join(secrets_dir, name))
        if os.path.isfile(path):
            secrets[name] = { 'file': path }

    return secrets

def get_registry(config):
    registry = config['cppdock'].get('registry')
    return registry or '127.0.0.1:5000'

def get_registry_tag(config, tag_name):
    return get_registry(config) + '/' + tag_name

def get_service_type(config, platform_name):
    type = get_platform_type(config, platform_name)
    if type == 'service' or not type.startswith('service:'):
        return None
    return type.split(':', 1)[1]

def get_domain(config, platform_name):
    platform = get_platform_config(config, platform_name)
    return platform.get('domain') or config['cppdock'].get('domain')

def get_build_type(config, platform_name):
    platform = get_platform_config(config, platform_name)
    default = config.get('__default_build_type', 'debug')
    return platform.get('build_type') \
        or config['cppdock'].get('build_type', default)

def get_release_target(config, platform_name):
    platform = get_platform_config(config, platform_name)
    return platform.get('release_target', 'install')
    

def dev_cluster_stop(config):
    stack_name  = get_stack_name(config)
    subprocess.check_output(['docker', 'stack', 'rm', stack_name])

def dev_cluster_clean_containers(config):
    stack_name  = get_stack_name(config)
    ids = get_container_ids(stack_name)
    if not ids:
        return
    subprocess.check_output(['docker', 'container', 'rm', '--force'] + ids)

def dev_cluster_clean_volumes(config):
    stack_name  = get_stack_name(config)
    names = get_dev_compose_object(config)['volumes'].keys()
    name_set = set(map(lambda x: stack_name + '_' + x, names))
    names = list(name_set.intersection(get_volume_names(stack_name)))
    if not names:
        return
    subprocess.check_output(['docker', 'volume', 'rm'] + names)

def get_container_ids(name):
    # docker ps -a --filter "name=sc2_nbdl_test_server" --format="{{.ID}}"
    # filter network to prevent accidentally remove similarly named containers
    filter_name = 'name={}'.format(name)
    filter_network = 'network={}'.format(name + '_default')
    return subprocess.check_output(['docker', 'ps', '-a',
                                    '--filter', filter_name,
                                    '--filter', filter_network,
                                    '--format', '{{.ID}}'
                                   ]).split()

def get_volume_names(name):
    #docker volume ls --filter "name=sc2_nbdl_test" --format="{{.Name}}"
    filter_name = 'name={}'.format(name)
    return subprocess.check_output(['docker', 'volume', 'ls',
                                    '--filter', filter_name,
                                    '--format', '{{.Name}}'
                                   ]).split()

def get_machine_list(config):
    if config.get('machine_list'):
        return config['machine_list']

    machines = config.get('machines', {}).items()
    project_name = get_project_name(config)
    machine_name = lambda x: (project_name + '_' + x).replace('_', '-')
    is_manager = lambda x: x[1].get('manager') == True

    result = [(machine_name(name), values) for (name, values) in machines]
    result.sort(key = is_manager, reverse = True)

    # prevent further access to unprocessed data
    if machines:
        del config['machines']

    return result

def get_machine_names(config):
    return (x for (x, y) in get_machine_list(config))

# TODO maybe use this when we actuall do stuff that needs to be
#      applied to all machines such as rsyncing source code
# for_every_machine or replace with some kind of iterator
def for_every_machine(config, fn):
    for (name, values) in config['machine_list']:
        set_machine(name, values)
        fn(name, values)

def set_master_machine(config):
    machines = get_machine_list(config)
    if len(machines) > 0:
        set_machine(*(machines[0]))

def get_master_machine_name(config):
    machines = get_machine_list(config)
    machine_names = [x[0] for x in machines]
    assert len(machine_names) > 0
    return machine_names[0]

def set_machine(name, values):
    x = machine_env(name, values)
    env = os.environ

    print("Setting docker host to machine: " + name)
    def set_env(name):
        value = x.get(name)
        if value:
            env[name] = value
        else:
            del env[name]

    set_env('DOCKER_HOST')
    set_env('DOCKER_TLS_VERIFY')
    set_env('DOCKER_CERT_PATH')
    set_env('DOCKER_MACHINE_NAME')
    print('Using docker host: ' + env['DOCKER_HOST'])

def machine_env(name, values):
    if values.get('env'):
        return values['env']
    else:
        raw = subprocess.check_output(['docker-machine', 'env', name])
          
    return dict(re.findall('export ([A-Z_]+)="([^"]+)"', raw))

def machine_start_all(config):
    name_list = list(get_machine_names(config))
    subprocess.check_output(['docker-machine', 'start'] + name_list)
    subprocess.check_output(['docker-machine', 'regenerate-certs',
                                               '--force'] + name_list)

def machine_stop_all(config):
    names = get_machine_names(config)
    subprocess.check_output(['docker-machine', 'stop'] + list(names))

def machine_rm_all(config):
    names = get_machine_names(config)
    subprocess.check_output(['docker-machine', 'rm', '-y'] + list(names))

def machine_create_all(config):
    # The first machine is the host
    xs = get_machine_list(config)
    assert len(xs) > 0, '"machines" must be specified for cluster operations'

    machine_create_master(*(xs[0]))
    def join_new(name, values):
        join_token = machine_join_token(name_values)
        machine_create(name, values, join_token)

    [join_new(name, values) for (name, values) in xs[1:]]

def machine_create_master(name, values):
    # The first machine that starts the swarm
    machine_create(name, values)
    set_machine(name, values)
    subprocess.check_output(['docker', 'swarm', 'init'])
    dev_registry_create()

def machine_create(name, values, join_token = None):
    labels = values.get('labels', {}).items()
    ports = values.get('ports', [80, 443])
    # ops is a list of lists to be chained
    ops = []
    # NOTE We are always setting `swarm-master`
    #      because a node could be promoted.
    #      (I think it just exposes a port)
    ops += [['--swarm', '--swarm-master']]
    ops += [['--driver', values['driver']]]
    ops += [['--{}'.format(x[0]), x[1]] for x in values.get('options', [])]
    ops += [['--engine-label', '{0}={1}'.format(k, v)] for k, v in labels]
    if join_token:
        ops += [['--swarm-discovery', 'token://' + join_token]]
    if values['driver'] == 'amazonec2':
        ops += [['--amazonec2-open-port', str(x)] for x in ports]
    ops_flat = [x for x in itertools.chain.from_iterable(ops)]
    subprocess.check_output(['docker-machine', 'create'] + ops_flat + [name])

def machine_join_token(name, values):
    kind = 'manager' if values.get('manager') else 'worker'
    return subprocess.check_output(['docker', 'swarm',
                                    'join-token', '-q', kind])

def machine_rsync(name, src_path, dest_path):
    machine = machine_inspect(name);
    ip_address = machine_ip_address(name)
    driver = machine['Driver']
    ssh_key_path = driver['SSHKeyPath']
    node_path = "{user}@{ip_address}:{dest_path}".format(
            user        = driver['SSHUser'],
            ip_address  = ip_address,
            dest_path   = dest_path)
    print('Syncing source code to ' + node_path)
    ssh_command = ' '.join([
        'ssh -i {}'.format(ssh_key_path),
            '-o StrictHostKeyChecking=no',
            '-o UserKnownHostsFile=/dev/null',
            '-o LogLevel=quiet'
            ])
    subprocess.check_output(['rsync', '--recursive',
                                      '--progress', 
                                      '--times', 
                                      '-e', ssh_command,
                                      src_path, node_path])

def machine_inspect(name):
    result_s =  subprocess.check_output(['docker-machine', 'inspect', name])
    return json.loads(result_s)

def machine_ip_address(name):
    result_s =  subprocess.check_output(['docker-machine', 'ip', name])
    return result_s.strip()

def dev_registry_create():
    registry_name = 'cppdock-registry'
    subprocess.check_output(['docker', 'service', 'create', '--name',
                             'cppdock-registry', '--publish', '5000:5000',
                             'registry'])

def dev_registry_rm():
    registry_name = 'cppdock-registry'
    subprocess.check_output(['docker', 'service', 'rm', 'cppdock-registry'])

def calculate_source_path(config, config_paths):
    # should be called AFTER config rewrite
    source = config["source"]
    if source and "path" in source:
        path = source["path"]
    else:
        path = config_paths[0]

    if path and not os.path.isdir(path):
        path = os.path.dirname(path)
    return path

def calculate_allow_sync(config):
    # should be called AFTER config rewrite
    # if source path is explicitly null then
    # forbid rsync no matter what
    source = config.get('source')
    if source and 'path' in source and source['path'] != None:
        return source.get('allow_sync', True)

    return False

def get_build_source_path(config):
    # do we imply source path when we set cppdock.base_config?
    path = config['source']['path']
    if is_cluster_mode(config):
        path = path or os.path.abspath('./source')
        path = cluster_get_sync_path(path)
    assert path, 'Unable to create source path for build'
    return path

def is_allowed_cluster_sync(config):
    return config['source']['allow_sync']

def is_verbose_mode():
    return True;


dispatch(os.sys.argv[1:2], os.sys.argv[2:])
